---
title: "AN588-Week-5"
author: "Jess Martin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Module 09 (Introduction to Statistical Inference)

**Preliminaries:**
Install this package in R: {curl}

**Objectives:**
The objective of this module is to continue our discussion of statistical inference and introduce basic hypothesis testing from a frequentist/classical statistics approach.

**Standard Errors and Confidence Intervals:**
*To recap some of what we have covered in the last two modules, standard errors are key to calculating confidence intervals and to basic inferential statistics.

*In Module 7, we calculated confidence intervals for one of our estimates of a population parameter (the population mean, an estimand), based on a sample statistic (the sample mean, our estimator). Let’s revist that process…

*The general way to define a confidence interval based on data from a sample is as the value of the statistic being considered (e.g., the mean) ± some critical value × the standard error of the statistic.

*The critical value is derived from the standardized version of a sampling distribution (e.g., the normal distribution) that corresponds the quantile limits we are interested in. For example, for the 95% CI around the mean, the critical value corresponds the range of quantiles above and below which we expect to see only 5% of the distribution of statistic values. This is equivalent to the ± 1 - (α/2) quantiles, where α=0.05, i.e., the ± 0.975 quantile that we have used before for calculating 95% CIs.

*The standard error is the standard deviation of the sampling distribution, which, as noted above, is often estimated from the sample itself as σ
/sqrt(n) but can also be calculated directly from the population standard deviation, if that is known.

*Recall that in Module 8, we created a vector, v, containing 1000 random numbers selected from a normal distribution with mean 3.5 and standard deviation 4. We then calculated the mean, standard deviation, and standard error of the mean (SEM) based on a sample of 30 observations drawn from that vector, and we used the normal distribution to characterize the quantiles associated with the central 95% of the distribution to define the upper and lower bounds of the 95% CI.

```{r, part 1}
n <- 1000
mu <- 3.5
sigma <- 4
v <- rnorm(n, mu, sigma)
s <- sample(v, size = 30, replace = FALSE)
m <- mean(s)
m
```
```{r, part 2}
sd <- sd(s)
sd
```
```{r, part 3}
sem <- sd(s)/sqrt(length(s))
sem
```
```{r, part 4}
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci <- c(lower, upper)
```

**The Central Limit Theorem:**
*Thus far, our construction of CIs has implicitly taken advantage of one of the most important theorems in statistics, the Central Limit Theorem. The key importance of the CLT for us is that it states that the distribution of averages (or sums or other summary statistics…) of iid (independent and identically distributed) random variables becomes normal as the sample size increases. It is this fact that allows us to have a good sense of the mean and distribution of average events in a population even though we only observe one set of events and do not know what actual population distribution is. In fact, the CLT says nothing about the probability distribution for events in the original population, and that is exactly where its usefulness lies… that original probability distribution can be normal, skewed, all kinds of odd!

*But we can nonetheless assume normality for the distribution of sample mean (or of the sum or mode, etc…) no matter what kind of probability distribution characterizes the initial population, as long as our sample size is large enough and our samples are independent. It is thus the CLT that allows us to make inferences about a population based on a sample.

*Just to explore this a bit, let’s do some simulations. We are going to take lots of averages of samples from a particular non-normal distribution and then look at the distribution of those averages. Imagine we have some event that occurs in a population according to some probability mass function like the Poisson where we know λ=14. Recall, then, that the expectations of μ and σ2 for the Poisson distribution are both=λ.

*Now let’s imagine taking a bunch of samples of size 10 from this population. We will take 1000 random samples of this size, calculate the average of each sample, and plot a histogram of those averages… it will be close to normal, and the standard deviation of the those average - i.e., of the sampling distribution - should be roughly equal to the estimated standard error, the square root of (λ/n). [Recall that λ is the expected variance, so this is simply the square root of (expected variance / sample size)]

```{r, part 5}
lambda <- 14
n <- 10
pop_se <- sqrt(lambda/n)  # the estimated SE
pop_se
```
```{r, part 6}
x <- NULL
for (i in 1:1000) {
    x[i] <- mean(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda +
    4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
```

```{r, part 7}
sd <- sd(x)  # st dev of the sampling distribution
sd
```
```{r, part 8}
qqnorm(x)
qqline(x)
```

*Let’s do this for samples of size 100, too. We see that the mean stays the same, the distribution is still normal, but the standard deviation - the spread - of the sampling distribution is lower.

```{r, part 9}
n <- 100
pop_se <- sqrt(lambda/n)  # the estimated SE
pop_se
```
```{r, part 10}
x <- NULL
for (i in 1:1000) {
    x[i] <- mean(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda +
    4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
```
```{r, part 11}
sd <- sd(x)  # st dev of the sampling distribution
sd
```
```{r, part 12}
qqnorm(x)
qqline(x)
```

*We can convert these distributions to standard normals by subtracting off the expected population mean (λ) and dividing by the standard error of the mean (an estimate of the standard deviation of the sampling distribution) and then plotting a histogram of those values along with a normal curve.

```{r, part 13}
curve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.8))
z <- (x - lambda)/pop_se
hist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE,
    add = TRUE)
```

*Pretty normal looking, right?

Here’s an example of the CLT in action using sum() instead of mean()…

```{r, part 14}
n <- 100
x <- NULL
for (i in 1:1000) {
    x[i] <- sum(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(min(x), max(x), length.out = 20), probability = TRUE)
```

*Again, pretty normal looking!

*Take Home Points:

*[1] The CLT states that, regardless of the underlying distribution, the distribution of averages (or sums or standard deviations, etc…) based on a large number of independent, identically distributed variables:

*will be approximately normal,

*will be centered at the population mean, and – will have a standard deviation roughly equal to the standard error of the mean.

*Additionally, it suggests that variables that are expected to be the sum of multiple independent processes (e.g., measurement errors) will also have distributions that are nearly normal.

*[2] Taking the mean and adding and subtracting the relevant standard normal quantile ×  the standard error yields a confidence interval for the mean, which gets wider as the coverage increases and gets narrower with less variability or larger sample sizes.

*[3] As sample size increases, the standard error of the mean decreases and the distribution becomes more and more normal (i.e., has less skew and kurtosis, which are higher order moments of the distribution).

*For a nice interactive simulation demonstrating the Central Limit Theorem, check out this cool website.

**Confidence Intervals for Sample Proportions:**
*So far, we’ve talked about CIs for sample means, but what about for other statistics, e.g., sample proportions for discrete binary variables. For example, if you have a sample of n trials where you record the success or failure of a binary event, you obtain an estimate of the proportion of successes, x/n. If you perform another n trials, the new estimate will vary in the same way that averages of a continuous random variable (e.g., zombie age) will vary from sample to sample. Taking a similar approach as above, we can generate confidence intervals for variability in the proportion of successes across trials.

*Recall from our discussion of discrete random binary variables that the expectation for proportion of successes, which we will denote as π
(where π, for “proportion”, is analogous to μ, for “mean”) is simply the average number of successes across multiple trials.

*The expected sampling distribution for the proportion of successes is approximately normal centered at π and its standard deviation is estimated by sqrt(π(1−π)/n), which is, essentially, the standard error of the mean: it is the square root of (the expected variance / sample size). As above for μ, if we do not already have a population estimate for π, we can estimate this from a sample as  = x/n.

*Note: this expectation based on an approximation of the normal holds true only if both n×π and n×(1−π) are greater than roughly 5, so we should check this when working with proportion data.

**CHALLENGE**
*Suppose a polling group in the Massachusetts is interested in the proportion of voting-age citizens in their state that already know they will vote for Elizabeth Warren in the upcoming November 5, 2024 general elections (don’t forget to vote!). The group obtains a yes or no answer from 1000 suitable randomly selected individuals. Of these individuals, 856 say they know they’ll vote for Senator Warren. How would we characterize the mean and variability associated with this proportion?

```{r, part 15}
n <- 1000
x <- 856
phat <- x/n  # our estimate of pi
phat
```
*Are n×π and n×(1−π) both > 5? Yes!

```{r, part 16}
n * phat
```
```{r, part 17}
n * (1 - phat)
```
```{r, part 18}
pop_se <- sqrt((phat) * (1 - phat)/n)
```

*So, what is the 95% CI around our estimate of the proportion of people who already know how they will vote?

```{r, part 19}
curve(dnorm(x, mean = phat, sd = pop_se), phat - 4 * pop_se, phat + 4 * pop_se)
upper <- phat + qnorm(0.975) * pop_se
lower <- phat - qnorm(0.975) * pop_se
ci <- c(lower, upper)
polygon(cbind(c(ci[1], seq(from = ci[1], to = ci[2], length.out = 1000), ci[2]),
    c(0, dnorm(seq(from = ci[1], to = ci[2], length.out = 1000), mean = phat,
        sd = pop_se), 0)), border = "black", col = "gray")
abline(v = ci)
abline(h = 0)
```

**Small Sample Confidence Intervals:**
*Thus far, we have discussed creating a confidence interval based on the CLT and the normal distribution, and our intervals took the form:

mean ± Z (the quantile from the standard normal curve) × standard error of the mean

*But, when the size of our sample is small (n < 30), instead of using the normal distribution to calculate our CIs, statisticians typically use a different distribution to generate the relevant quantiles to multiply the standard error by… the t distribution (a.k.a., Gosset’s t or Student’s t distribution).

*Note that this is the typical case that we will encounter, as we often do not have information about the population that a sample is drawn from.

*The t distribution is a continuous probability distribution very similar in shape to the normal is generally used when dealing with statistics (such as means and standard deviations) that are estimated from a sample rather than known population parameters. Any particular t distribution looks a lot like a normal distribution in that it is bell-shaped, symmetric, unimodal, and (if standardized) zero-centered.

*The choice of the appropriate t distribution to use in any particular statistical test is based on the number of degrees of freedom (df), i.e., the number of individual components in the calculation of a given statistic (such as the standard deviation) that are “free to change”.

*We can think of the t distribution as representing a family of curves that, as the number of degrees of freedom increases, approaches the normal curve. At low numbers of degrees of freedom, the tails of the distribution get fatter.

*Confidence intervals based on the t distribution are of the form:

mean ± T (the quantile from the t distribution) × standard error of the mean

*The only change from those based on the normal distribution is that we’ve replaced the Z quantile of the standard normal with a T quantile.

Let’s explore this a bit…

*Recall that a standard normal distribution can be generated by normalizing our sample (subtracting the population mean from each observation and then dividing all of these differences by the population standard deviation)…

(mean(x)-μ)/σ

*If we subtract the population mean from each observation but then divide each difference, instead, by the standard error of the mean, (mean(x)-μ
)/SEM, the result is not a normal distribution, but rather a t distribution! We are taking into account sample size by dividing by the standard error of the mean rather than the population standard deviation.

*The code below plots a standard normal distribution and then t distributions with varying degrees of freedom, specified using the df= argument. As for other distributions, R implements density, cumulative probability, quantile, and random functions for the t distribution.

```{r, part 20}
mu <- 0
sigma <- 1
curve(dnorm(x, mu, 1), mu - 4 * sigma, mu + 4 * sigma, main = "Normal Curve=red\nStudent's t=blue",
    xlab = "x", ylab = "f(x)", col = "red", lwd = 3)
for (i in c(1, 2, 3, 4, 5, 10, 20, 100)) {
    curve(dt(x, df = i), mu - 4 * sigma, mu + 4 * sigma, main = "T Curve", xlab = "x",
        ylab = "f(x)", add = TRUE, col = "blue", lty = 5)
}
```

*The fatter tails of the t distibution lead naturally to more extreme quantile values given a specific probability than we would see for the normal distribution. If we define a CI based on quantiles of the t distribution, they will be correspondingly slightly wider than those based on the normal distribution for small values of df.

*We can see this as follows. Recall that above we estimated the 95% CI for a sample drawn from a normal distribution as follows:

```{r, part 21}
n <- 1e+05
mu <- 3.5
sigma <- 4
x <- rnorm(n, mu, sigma)
sample_size <- 30
s <- sample(x, size = sample_size, replace = FALSE)
m <- mean(s)
m
```
```{r, part 22}
sd <- sd(s)
sd
```
```{r, part 23}
sem <- sd(s)/sqrt(length(s))
sem
```
```{r, part 24}
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_norm <- c(lower, upper)
ci_norm
```
*Now, let’s look at the CIs calculated based using the t distribution for the same sample size. For sample size 30, the difference in the CIs is negligible.

```{r, part 25}
lower <- m - qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_t <- c(lower, upper)
ci_t
```
*However, if we use a sample size of 5, the CI based on the t distribution is much wider.

```{r, part 26}
sample_size <- 5
s <- sample(x, size = sample_size, replace = FALSE)
m <- mean(s)
m
```
```{r, part 27}
sd <- sd(s)
sd
```
```{r, part 28}
sem <- sd(s)/sqrt(length(s))
sem
```
```{r, part 29}
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_norm <- c(lower, upper)
ci_norm
```
```{r, part 30}
lower <- m - qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_t <- c(lower, upper)
ci_t
```
## Module 10 (Classical Hypothesis Testing)

**Objectives:**
*The objective of this module is to continue our discussion of classical hypothesis testing from a frequentist statistics approach.

**Null and Alternative Hypotheses:**
*Classical or frequentist hypothesis testing (a.k.a. parametric statistics) involves formally stating a claim - the null hypothesis - which is then followed up by statistical evaluation of the null versus an alternative hypotheses. The null hypothesis is interpreted as a baseline hypothesis and is the claim that is assumed to be true. The alternative hypothesis is the conjecture that we are testing.

*We need some kind of statistical evidence to reject the null hypothesis in favor of an alternative hypothesis. This evidence is, in classicical frequentist approaches, some measure of how unexpected it would be for the sample to have been drawn from a given null distribution.

*H0 = null hypothesis = a sample statistic shows no deviation from what is expected or neutral

*HA = alternative hypothesis = a sample statistic deviates more than expected by chance from what is expected or neutral

*We can test several different comparisons between H0 and HA.

*HA > H0, which constitutes an “upper one-tailed test (i.e., our sample statistic is greater than that expected under the null)

*HA < H0, which constitutes an “lower one-tailed test (i.e., our sample statistic is less than that expected under the null)

*HA ≠ H0, which constitutes an “two-tailed test (i.e., our sample statistic is different, maybe greater maybe less, than that expected under the null)

*There are then four possible outcomes to our statistical decision:

*In classical frequentist (a.k.a. parametric) inference, we perform hypothesis testing by trying to minimize our probability of Type I error… we aim for having a high bar for falsely rejecting the null (e.g., for incorrectly finding an innocent person guilty). When we set a high bar for falsely rejecting the null, we lower the bar for falsely ‘accepting’ (failing to reject) the null (e.g., for concluding that a guilty person is innocent).

*To do any statistical test, we typically calculate a test statistic based on our data, which we compare to some appropriate standardized sampling distribution to yield a p value.

*The p value = the probability of our obtaining a test statistic that is as high or higher than our calculated one by chance, assuming the null hypothesis is true.

*The test statistic basically summarizes the “location” of our data relative to an expected distribution based on our null model. The particular value of our test statistic is determined by both the difference between the original sample statistic and the expected null value (e.g., the difference between the mean of our sample and the expected population mean) and the standard error of the sample statistic. The value of the test statistic (i.e., the distance of the test statistic from zero) and the shape of the null distribution are the sole drivers of the smallness of the p value. The p value effectively represents the area under the sampling distribution associated with test statistic values as or more extreme than the one we observed.

*We compare the p value associated with our test statistic to some significance level, α, typically 0.05 or 0.01, to determine whether we reject or fail to reject the null. If p < α, we decide that there is sufficient statistical evidence for rejecting H0.

*How do we calculate the p value?

1. Specify a test statistic (e.g., the mean)

2. Specify our null distribution

3. Calculate the tail probability, i.e., the probability of obtaining a statistic (e.g., a mean) as or more extreme than was observed assuming that null distribution.

**Testing Sample Means: One Sample Z and T Tests:**
*Let’s do an example where we try to evaluate whether the mean of a single set of observations is significantly different than expected under a null hypothesis… i.e., this is a ONE-SAMPLE test.

*Let’s do an example where we try to evaluate whether the mean of a single set of observations is significantly different than expected under a null hypothesis… i.e., this is a ONE-SAMPLE test.

```{r, part 31}
library(curl)
```

```{r, part 32}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/vervet-weights.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
```{r, part 33}
mean(d$weight)
```
*What is our H0? The mean adult weight of vervet monkeys in 2016 is equal to or less than 4.9 kilograms.
*What is our HA? The mean adult weight of vervet monkeys in 2016 is greater than 4.9 kilograms.
*What is the hypothesis we want to test? Is it two-tailed? Upper-tailed? Lower-tailed? The hypothesis we want to test is one-tailed (specifically, upper-tailed), as we are interested in whether the mean weight is greater than the expected value.
*Calculate the mean, standard deviation, and SEM from our sample.

```{r, part 34}
mu <- 4.9
x <- d$weight
m <- mean(x)
s <- sd(x)
n <- length(x)
sem <- s/sqrt(n)
```

*Our test statistic takes a familiar form… it is effectively the standard normalized position of our sample mean in a distribution centered around the expected population mean.


with

df=n-1

where:

*x = mean of sample observations 
*μ = expected mean
*s = sample standard deviation
*n = number of sample observations

Or, to use our variables from above…

```{r, part 35}
z <- (m - mu)/sem
z
```
*In this case, z is a quantile… the estimated number of standard errors of the mean away from the population mean that our sample falls.

*If we then want to see if z is significant, we need to calculate the probability of seeing a deviation from the mean as high or higher than this by chance. To do this, we can use the pnorm() function. Because we have converted our sample mean to the standard normal scale, the mean= and sd= arguments of pnorm() are the defaults of 0 and 1, respectively.

*We want the probability of seeing a z this large or larger by chance.

```{r, part 36}
p <- 1 - pnorm(z)
p
```
```{r, part 37}
p <- pnorm(z, lower.tail = FALSE)
p
```
*However, as noted above, our sample size from a population is typically limited. So, instead of using the normal distribution to determine the p value of our statistic, we should use the t distribution, which, as we’ve seen, has slightly fatter tails. The statistic and process is exactly the same, though, as for the normal distribution.

```{r, part 38}
p <- 1 - pt(z, df = n - 1)
p
```
```{r, part 39}
p <- pt(z, df = n - 1, lower.tail = FALSE)
p
```
*R has built into it a single function, t.test(), that lets us do all this in one line. We give it our data and the expected population mean, μ, along with the kind of test we want to do.

```{r, part 40}
t <- t.test(x = x, mu = mu, alternative = "greater")
t
```
*Note that we can also use the t.test() function to calculate CIs based on the t distribution for us easily!

```{r, part 41}
lower <- m - qt(1 - 0.05/2, df = n - 1) * sem
upper <- m + qt(1 - 0.05/2, df = n - 1) * sem
ci <- c(lower, upper)
ci  # by hand
```
```{r, part 42}
t <- t.test(x = x, mu = mu, alternative = "two.sided")
ci <- t$conf.int
ci  # using t test
```
*So our conclusion, then, would be to reject the H0 that the weights of the sample of monkeys from the 2016 trapping season come from the same distribution as the weights of monkeys from previous trapping seasons based on the average, since the average from previous seasons falls outside the 95% confidence interval for the t distribution based on the sample average from the 2016 trapping season.

*In other words, the 2016 trapping season has vervet monkeys whose weights are significantly heavier than the average from previous trapping seasons (p < 0.01).

**CHALLENGE 1**
*Adult lowland woolly monkeys are reported to have an average body weight of 7.2 kilograms. You are working with an isolated population of woolly monkeys from the Peruvian Andes that you think may be a different species from the lowland form, and you collect a sample of 15 weights from adult individuals at that site. From your sample, you calculate a mean of 6.43 kilograms and a standard deviation of 0.98 kilograms.

*Perform a hypothesis test of whether body weights in your population are different from the reported average for lowland woolly monkeys by setting up a “two-tailed” hypothesis, carrying out the analysis, and interpreting the p value (assume the significance level is α
=0.05). Your sample is < 30, so you should use the t distribution and do a t test. Do your calculations both by hand and using the t.test() function and confirm that they match.

```{r, part 43}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/woolly-weights.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
```{r, part 44}
x <- d$weight
m <- mean(x)
s <- sd(x)
n <- length(x)
sem <- s/sqrt(n)
mu <- 7.2
t <- (m - mu)/sem
t
```
```{r, part 45}
alpha <- 0.05
crit <- qt(1 - alpha/2, df = n - 1)  # identify critical values
test <- t < -crit || t > crit  # boolean test as to whether t is larger than the critical value at either tail
test <- abs(t) > crit
t.test(x = x, mu = mu, alternative = "two.sided")
```
*Based on our conclusions for the vervet sample, what do the results of this comparison across woolly monkey populations mean?
So our conclusion, then, would be to reject the H0 and note that woolly monkey weights (based off this isolated population) are significantly lighter than the average body weight previously reported.

**Comparing Sample Means: Two Sample Z and T Tests:**
*Sometimes we want to compare two groups of measurements to one another, which boils down to a hypothesis test for the difference between two means, μ1 and μ2. The null hypothesis is that the difference between the means is zero.

*Before getting to the appropriate test, there are a couple of things that we need to consider:

[1] How, if at all, are the two samples related to one another? Sometimes we may have PAIRED samples (e.g., the same individuals before and after some treatment) and sometimes the samples are UNPAIRED or INDEPENDENT (e.g., weights for different samples of black-and-white colobus monkeys collected in the rainy versus dry seasons).

[2] Are the variances in the two samples roughly equal or not? E.g., if we are comparing male and female heights, are the variances comparable?

**Samples with Unequal Variances:**
*For the most generic case, where the two samples are independent and we cannot assume the variances of the two samples are equal, we can do what is called Welch’s t test where our test statistic is:

where:

*x1 and x2 = means of observations in each sample group
*μ = expected difference in means between sample groups under the null hypothesis, which is usually zero
*s1 and s2 = standard deviations of each sample group
*n1 and n2 = numbers of observations in each sample group

**CHALLENGE 2**
*Let’s do an example. Load in a file of black-and-white colobus weights and examine the str() of the file. Then, create 2 vectors, x and y, for male and female weights. Plot these in boxplots side by side and then calculate the mean, sd, and sample size for both males and females.

```{r, part 46}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/colobus-weights.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
```{r, part 47}
x <- d$weight[d$sex == "male"]
y <- d$weight[d$sex == "female"]
par(mfrow = c(1, 2))
boxplot(x, ylim = c(4.5, 8), main = "Weight (kg)", xlab = "Males")
boxplot(y, ylim = c(4.5, 8), main = "Weight (kg)", xlab = "Females")
```
```{r, part 48}
m1 <- mean(x)
m2 <- mean(y)
mu <- 0  # you could leave this out... the default argument value is 0
s1 <- sd(x)
s2 <- sd(y)
n1 <- length(x)
n2 <- length(y)
```

*Now calculate the t statistic and test the two-tailed hypothesis that the sample means differ.

```{r, part 49}
t <- (m2 - m1 - mu)/sqrt(s2^2/n2 + s1^2/n1)
t
```
```{r, part 50}
alpha <- 0.05
crit <- qt(1 - alpha/2, df = n - 1)  # identify critical values
crit
```
```{r, part 51}
test <- t < -crit || t > crit  # boolean test
test <- abs(t) > crit
test
```
```{r, part 52}
df <- (s2^2/n2 + s1^2/n1)^2/((s2^2/n2)^2/(n2 - 1) + (s1^2/n1)^2/(n1 - 1))
df
```
*Do the same using the t.test() function.

```{r, part 53}
t <- t.test(x = x, y = y, mu = 0, alternative = "two.sided")
t
```
*How do we interpret these results?
-The p-value (1.023e-12) is extremely small, indicating strong evidence against the null hypothesis (that the true difference in means is 0). This suggests that the means of the two groups (x and y) are significantly different from each other.
-The t-statistic (11.46) is highly positive, indicating a substantial difference between the means of x and y.
-The 95% confidence interval (1.191186 to 1.706814) for the difference in means does not include 0. This further supports the conclusion that there is a significant difference between the means of the two groups.

**Samples with Equal Variances:**
*There’s a simpler t statistic we can use if the variances of the two samples are more or less equal.

*x1 and x2 = means of observations in each sample group
*μ = expected difference in means between sample groups (usually set to zero)
*sp = pooled sample standard deviation
*n1 and n2 = numbers of observations in each sample group

```{r, part 54}
s <- sqrt((((n1 - 1) * s1^2) + ((n2 - 1) * s2^2))/(n1 + n2 - 2))
t <- (m2 - m1 - mu)/(sqrt(s^2 * (1/n1 + 1/n2)))
t
```

```{r, part 55}
df <- n1 + n2 - 2
df
```
```{r, part 56}
t <- t.test(x = x, y = y, mu = 0, var.equal = TRUE, alternative = "two.sided")
t
```
*A crude test for equality of variances is to divide the larger by the smaller and if the result is < 2, you can go ahead and used the pooled variance version of the test (which has many fewer degrees of freedom).

*In our case, we cannot, since the ratio of variances exceeds 2…

```{r, part 57}
var(x)/var(y)
```
*We can use the var.test() function to conduct an actual statistical test on the ratio of variances, which compares the ratio test statistic we just calculated to an F distribution. The F distribution is often used to model ratios of random variables and thus is useful in regression applications and, as here, for testing whether variances from two samples are different. It is dependent upon the specification of a pair of degrees of freedom values supplied as the arguments df1= and df2= (or inferred from the number of observations in each sample).

*Below, the results of var.test() are saved to a variable. Calling the variable provides a brief descriptive summary.

```{r, part 58}
vt <- var.test(x, y)
vt
```
**Paired Samples:**
*For a paired sample test, the null hypothesis is that the mean of individual paired differences between the two samples (e.g., before and after) is zero.

*d = mean of difference between paired samples
*μ = expected mean difference between paired samples (usually set to zero)
*sd = standard deviation in the set of differences between paired samples
*n = number of sample pairs

*Again, note that μ here is the expected difference between the means under the null hypothesis, which is zero, and we are dividing by the standard error of the mean for the set of differences between pairs.

**CHALLENGE 3**
*Let’s play with a sample… IQs of individuals taking a certain statistics course before and after a lecture on significance testing. Load in the iqs.csv data file, look at it, plot a barchart of values before and after and construct a paired t test to evaluate the means before and after.

```{r, part 59}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/iqs.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
```{r, part 60}
x <- d$IQ.before - d$IQ.after
m <- mean(x)
mu <- 0  # can leave this out
s <- sd(x)
n <- length(x)
sem <- s/sqrt(n)
par(mfrow = c(1, 2))
boxplot(d$IQ.before, ylim = c(115, 145), main = "IQ", xlab = "Before")
boxplot(d$IQ.after, ylim = c(115, 145), main = "IQ", xlab = "After")
```
```{r, part 61}
t <- (m - mu)/sem
t
```
```{r, part 62}
alpha <- 0.05
crit <- qt(1 - alpha/2, df = n - 1)  # identify critical values
crit
```
```{r, part 63}
test <- t < -crit || t > crit  # boolean test
test
```
```{r, part 64}
t.test(x, df = n - 1, alternative = "two.sided")
```
*How do we interpret these results?
The p-value (0.08946) is greater than 0.05. This means that there is not strong enough evidence to reject the null hypothesis.

**Testing Sample Proportions: One Sample Z Test:**
*As we have seen, the sampling distribution of sample means for independent and identically distributed random variables is roughly normal (and, as shown by the CLT, this distribution increasingly approaches normal as sample size increases). Similarly, the sampling distribution for another kind of sample statistic, the number of “successes” x out of a series of k trials is also roughly normally distributed. If the true population proportion of “successes” is π, then the sampling distribution for the proportion of successes in a sample of size n is expected to be roughly normally distributed with mean = π and standard error = sqrt(π(1−π)/n).

*Let’s set up a simulation to show this…

*First we create a population of 500 “1”s and 500 “0”s, i.e., where π = 0.5…

```{r, part 65}
pop <- c(rep(0, 500), rep(1, 500))
```
*Now, we will take 1000 random samples of size n=10 from that population and calculate the proportion of “1”s in each sample…

```{r, part 66}
pi <- 0.5
x <- NULL
n <- 10
for (i in 1:1000) {
    x[i] <- mean(sample(pop, size = n, replace = FALSE))  # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!
}
m <- mean(x)
m
```
```{r, part 67}
s <- sd(x)
s
```
```{r, part 68}
pop_se <- sqrt(pi * (1 - pi)/n)
pop_se  # the se is an estimate of the sd of the sampling distribution
```
*The same is true if we create a population of 800 “1”s and 200 “0”s, i.e., where π = 0.8…

```{r, part 69}
pop <- c(rep(0, 800), rep(1, 200))
pi <- 0.8
x <- NULL
n <- 10
for (i in 1:1000) {
    x[i] <- mean(sample(pop, size = n, replace = FALSE))  # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!
}
m <- mean(x)
m
```

```{r, part 70}
s <- sd(x)
s
```
```{r, part 71}
pop_se <- sqrt(pi * (1 - pi)/n)
pop_se  # the se is an estimate of the sd of the sampling distribution
```
*This normal approximation is true as long as n is fairly large and π is not close to 0 or 1. One rule of thumb is to check that both n×π and n×(1−π) are greater than 5.

*With all this in mind, we can construct Z statistics for proportions like we constructed Z and T statistics for means and test those proportions for differences from an expected value or for differences between two sample proportions. The Z statistic for proportions takes the same general form as that for means…

*z = (observed statistic - expected statistic) / standard error

or,

*P = proportion in sample
*π = expected proportion
*n = number of observations in sample

**CHALLENGE 4**
*A neotropical ornithologist working in the western Amazon deploys 30 mist nets in a 100 hectare (ha) grid. She monitors the nets on one morning and records whether or not a she captures any birds in the net (i.e., a “success” or “failure” for every net during a netting session). The following vector summarizes her netting results:

```{r, part 72}
v <- c(0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
    1, 1, 0, 1, 0, 1, 1)
```
 
*Her netting success over the previous three seasons suggests that she should catch birds in 80% of her nets. This season, she feels, her success rate is lower than in previous years. Does her trapping data support this hypothesis?

*What is H0? The success rate of capturing birds in mist nets this season (π) is equal to or greater than 0.80 (the success rate of previous years).
*What is HA? The success rate of capturing birds in mist nets this season (π) is less than 0.80 (indicating a lower success rate than previous years).
*Are both n×π and n×(1−π) > 5? Yes, for a valid application of the normal approximation to the binomial distribution, both n×π and n×(1−π) should be greater than 5. In this case, n = 30, and if π = 0.8, then n×π = 30×0.8 = 24 and n×(1−π) = 30×(1−0.8) = 6, which satisfy this condition.
*Calculate z and the p value associated with z
*Calculate the 95% CI around p

```{r, part 73}
phat <- mean(v)
phat
```
```{r, part 74}
pi <- 0.8
n <- 30
z <- (phat - pi)/sqrt(pi * (1 - pi)/30)
z
```
```{r, part 75}
p <- pnorm(z, lower.tail = TRUE)
p
```
*We use the lower.tail=TRUE argument because we’re testing a lower-tailed one-tailed hypothesis. The 95% confidence interval can be estimated, based on the normal distribution, as follows:

```{r, part 76}
lower <- phat - qnorm(0.975) * sqrt(phat * (1 - phat)/30)
upper <- phat + qnorm(0.975) * sqrt(phat * (1 - phat)/30)
ci <- c(lower, upper)
ci
```
*This approach using quantiles of the standard normal distribution is but one method of calculating CIs for proportion data, and is the CI referred to as a Wald confidence interval. Note that the CI does not include the value of π… rather, π > is greater than the upper bound of the CI, suggesting that the observed success rate is indeed lower than in previous years.

*We can do the same test with a one-liner in R…

```{r, part 77}
pt <- prop.test(x = sum(v), n = length(v), p = 0.8, conf.level = 0.95, correct = FALSE,
    alternative = "less")
pt
```
*Note that the CI is different than we calculated based on the normal distribution… prop.test() implements a slightly different procedure for estimating the CI rather than basing this on the normal distribution and the CLT.

*How should she interpret these results? Was she right that her trapping results are off this year?
-The p-value (0.0031) is less than the significance level of 0.05, indicating strong evidence against the null hypothesis.
-The z-score (-2.739) is negative, which indicates that the observed proportion (0.6) is significantly lower than the assumed proportion (0.8) from previous years.
-The 95% confidence interval (0.425 to 0.775) for the population proportion π does not include the value 0.8. This suggests that the observed success rate is indeed lower than in previous years.
-Based on these results, she should interpret that her trapping results this year are significantly lower than in previous years- the evidence supports her feeling that the success rate of capturing birds in mist nets this season is lower compared to previous years.

**Comparing Sample Proportions: Two Sample Z Tests:**
*The Z statistic for the two sample test comparing proportions is also very similar to that for comparing means.

where:

*p1 and p2 = proportions of “successes” in each sample group
*π = expected difference in proportions between sample groups (usually set to zero)
*p∗ = pooled proportion
*n1 and n2 = numbers of observations in each sample group

**CHALLENGE 5**
*A biologist studying two species of tropical bats captures females of both species in a mist net over the course of week of nightly netting. For each species, the researcher records whether females are lactating or not. The two vectors below summarize the data for each species.
```{r, part 78}
v1 <- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
    1, 0)
v2 <- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
    0, 1, 1, 0, 1, 1, 1)
```

*Based on your mist netting data, do the species differ significantly in the proportion of lactating females during this trapping season? What are H0 and HA?
-H0: The proportion of lactating females in species 1 is equal to the proportion of lactating females in species 2 (p1=p2). 
-HA:The proportion of lactating females in species 1 is not equal to the proportion of lactating females in species 2 (p1 ≠ p2).

```{r, part 79}
pstar <- (sum(v1) + sum(v2))/(length(v1) + length(v2))
pstar
```
```{r, part 80}
phat1 <- mean(v1)
phat1
```
```{r, part 81}
phat2 <- mean(v2)
phat2
```
```{r, part 82}
pi <- 0
z <- (phat2 - phat1)/sqrt((pstar * (1 - pstar)) * (1/length(v1) + 1/length(v2)))
z
```
```{r, part 83}
p <- 1 - pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
p
```
```{r, part 84}
crit <- qnorm(1 - alpha/2)  # identify critical values
crit
```
```{r, part 85}
test <- p < -crit || p > crit  # boolean test
test
```
*We can use the prop.test() function to do this in one line.

```{r, part 86}
pt <- prop.test(x = c(sum(v2), sum(v1)), n = c(length(v2), length(v1)), alternative = "two.sided",
    correct = FALSE)
pt
```
*What are her results? Are the proportions of lactating females different across species during this trapping season?

-The p-value (0.2825) is greater than the significance level of 0.05.
-The calculated test statistic is approximately 1.075.
-The critical value for a two-tailed test (crit) is approximately 1.96.
-Since the p-value (0.2825) is greater than 0.05, thus we fail to reject the null hypothesis. There is not enough evidence to conclude that the proportions of lactating females differ significantly between the two species during this trapping season. The results suggest that the proportions are not significantly different.

**Summary of Z and T Tests:**
*Z and T tests are used to evaluate whether a given sample statistic (e.g., a mean or proportion) deviates significantly from what is expected under a null model or whether two samples statistics deviate significantly from one another.

*They are used for dealing with normally distributed, continuous variables or those that can be approximated closely by the normal distribution.

*We REJECT a H0 if the p value obtained for a given Z or T test statistic is < α

*CIs for our sample statistic are calculated as mean ± T(1−α/2) or Z(1−α/2) x SEM, and we can REJECT a H0 if the (1-α) CI around does not include the expected value of the statistic

*When the sample size > 30, or when we are dealing with proportions, we use Z quantiles for calculating CIs and p values, but for sample size < 30, we use T quantiles
